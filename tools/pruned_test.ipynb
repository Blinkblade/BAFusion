{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin_static: \n",
      "fc1.weight  :  torch.Size([20, 10])\n",
      "fc1.bias  :  torch.Size([20])\n",
      "fc2.weight  :  torch.Size([10, 20])\n",
      "fc2.bias  :  torch.Size([10])\n",
      "fc3.weight  :  torch.Size([1, 10])\n",
      "fc3.bias  :  torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义一个简单的模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 创建并初始化模型\n",
    "model = SimpleModel()\n",
    "\n",
    "# 模拟训练过程，随机初始化权重\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 假设模型已经训练过了，这里保存其状态字典\n",
    "torch.save({\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "}, 'original_model.pth')\n",
    "\n",
    "\n",
    "print(\"origin_static: \")\n",
    "for key,value in model.state_dict().items():\n",
    "    print(key, \" : \", value.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruned_static: \n",
      "fc1.weight  :  torch.Size([20, 10])\n",
      "fc1.bias  :  torch.Size([20])\n",
      "fc3.weight  :  torch.Size([1, 10])\n",
      "fc3.bias  :  torch.Size([1])\n",
      "Pruned model saved as 'pruned_bafpp.pth'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 模拟剪枝操作：删除 `fc2` 层的权重（假设剪枝）\n",
    "pruned_model = SimpleModel()\n",
    "pruned_state_dict = torch.load('original_model.pth')['state_dict']\n",
    "# 剪掉不需要的层（这里删除 `fc2` 层）\n",
    "pruned_state_dict.pop('fc2.weight')\n",
    "pruned_state_dict.pop('fc2.bias')\n",
    "\n",
    "# 保存剪枝后的权重\n",
    "torch.save({\n",
    "    'state_dict': pruned_state_dict,\n",
    "}, 'pruned_bafpp.pth')\n",
    "\n",
    "print(\"pruned_static: \")\n",
    "for key,value in pruned_state_dict.items():\n",
    "    print(key, \" : \", value.shape)\n",
    "\n",
    "print(\"Pruned model saved as 'pruned_bafpp.pth'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleModel(\n",
      "  (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (fc2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新定义一个模型，加载剪枝后的权重\n",
    "new_model = SimpleModel()\n",
    "\n",
    "# 加载剪枝后的权重\n",
    "checkpoint = torch.load('pruned_bafpp.pth')\n",
    "pruned_state_dict = checkpoint['state_dict']\n",
    "\n",
    "# 手动调整参数名称或去掉不必要的层\n",
    "pruned_state_dict = {k: v for k, v in pruned_state_dict.items() if k in new_model.state_dict()}\n",
    "\n",
    "# 更新模型\n",
    "new_model.load_state_dict(pruned_state_dict, strict=False)\n",
    "\n",
    "# 打印模型结构，查看哪些层被加载\n",
    "print(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('pruned_bafpp.pth')\n",
    "pruned_state_dict = checkpoint['state_dict']\n",
    "\n",
    "# 手动调整参数名称或去掉不必要的层\n",
    "pruned_state_dict = {k: v for k, v in pruned_state_dict.items() if k in model.state_dict()}\n",
    "\n",
    "# 更新模型\n",
    "model.load_state_dict(pruned_state_dict, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "原始模型结构:\n",
      "fc1: Linear(in_features=10, out_features=20, bias=True)\n",
      "fc2: Linear(in_features=20, out_features=10, bias=True)\n",
      "fc3: Linear(in_features=10, out_features=1, bias=True)\n",
      "\n",
      "原始模型已保存为 'original_model.pth'.\n",
      "\n",
      "剪枝后的模型结构:\n",
      "fc1: Linear(in_features=10, out_features=15, bias=True)\n",
      "fc2: Linear(in_features=15, out_features=10, bias=True)\n",
      "fc3: Linear(in_features=10, out_features=1, bias=True)\n",
      "\n",
      "剪枝后的模型权重已加载.\n",
      "剪枝后的模型已保存为 'pruned_model.pth'.\n",
      "\n",
      "加载剪枝后模型的结构:\n",
      "fc1: Linear(in_features=10, out_features=15, bias=True)\n",
      "fc2: Linear(in_features=15, out_features=10, bias=True)\n",
      "fc3: Linear(in_features=10, out_features=1, bias=True)\n",
      "\n",
      "比较原始模型和剪枝后的模型结构差异:\n",
      "原始模型的 fc1 输出特征数: 20\n",
      "剪枝后模型的 fc1 输出特征数: 15\n",
      "加载剪枝后模型的 fc1 输出特征数: 15\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "# 定义原始模型\n",
    "class OriginalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OriginalModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 定义剪枝后的模型（例如，减少fc2的输入特征数从20到15）\n",
    "class PrunedModel(nn.Module):\n",
    "    def __init__(self, pruned_fc1_out_features=15, pruned_fc2_in_features=15):\n",
    "        super(PrunedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, pruned_fc1_out_features)\n",
    "        self.fc2 = nn.Linear(pruned_fc2_in_features, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def print_model_structure(model, title):\n",
    "    print(f\"\\n{title}\")\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            print(f\"{name}: {module}\")\n",
    "\n",
    "def main():\n",
    "    # 1. 初始化并保存原始模型\n",
    "    original_model = OriginalModel()\n",
    "    print_model_structure(original_model, \"原始模型结构:\")\n",
    "    \n",
    "    # 模拟训练过程（这里只是随机初始化权重）\n",
    "    optimizer = optim.SGD(original_model.parameters(), lr=0.01)\n",
    "    \n",
    "    # 保存原始模型的权重\n",
    "    torch.save({\n",
    "        'state_dict': original_model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, 'original_model.pth')\n",
    "    print(\"\\n原始模型已保存为 'original_model.pth'.\")\n",
    "\n",
    "    # 2. 定义剪枝后的模型（例如，减少fc1的输出特征数从20到15）\n",
    "    pruned_fc1_out_features = 15\n",
    "    pruned_fc2_in_features = pruned_fc1_out_features  # fc2的输入特征数应与fc1的输出特征数一致\n",
    "    pruned_model = PrunedModel(pruned_fc1_out_features=pruned_fc1_out_features,\n",
    "                               pruned_fc2_in_features=pruned_fc2_in_features)\n",
    "    print_model_structure(pruned_model, \"剪枝后的模型结构:\")\n",
    "    \n",
    "    # 3. 复制并适配权重\n",
    "    original_state_dict = torch.load('original_model.pth')['state_dict']\n",
    "    pruned_state_dict = pruned_model.state_dict()\n",
    "\n",
    "    # 复制fc1的权重和偏置（保留前15个输出特征）\n",
    "    pruned_state_dict['fc1.weight'] = original_state_dict['fc1.weight'][:pruned_fc1_out_features, :]\n",
    "    pruned_state_dict['fc1.bias'] = original_state_dict['fc1.bias'][:pruned_fc1_out_features]\n",
    "\n",
    "    # 复制fc2的权重和偏置（保留前15个输入特征）\n",
    "    pruned_state_dict['fc2.weight'] = original_state_dict['fc2.weight'][:, :pruned_fc2_in_features]\n",
    "    pruned_state_dict['fc2.bias'] = original_state_dict['fc2.bias']\n",
    "\n",
    "    # 复制fc3的权重和偏置（保持不变）\n",
    "    pruned_state_dict['fc3.weight'] = original_state_dict['fc3.weight']\n",
    "    pruned_state_dict['fc3.bias'] = original_state_dict['fc3.bias']\n",
    "\n",
    "    # 4. 加载适配后的权重到剪枝后的模型\n",
    "    pruned_model.load_state_dict(pruned_state_dict)\n",
    "    print(\"\\n剪枝后的模型权重已加载.\")\n",
    "\n",
    "    # 5. 保存剪枝后的模型\n",
    "    torch.save({\n",
    "        'state_dict': pruned_model.state_dict(),\n",
    "    }, 'pruned_model.pth')\n",
    "    print(\"剪枝后的模型已保存为 'pruned_model.pth'.\")\n",
    "\n",
    "    # 6. 加载剪枝后的模型到新模型实例中\n",
    "    loaded_pruned_model = PrunedModel(pruned_fc1_out_features=pruned_fc1_out_features,\n",
    "                                     pruned_fc2_in_features=pruned_fc2_in_features)\n",
    "    checkpoint = torch.load('pruned_model.pth')\n",
    "    loaded_pruned_model.load_state_dict(checkpoint['state_dict'])\n",
    "    print_model_structure(loaded_pruned_model, \"加载剪枝后模型的结构:\")\n",
    "\n",
    "    # 7. 比较原始模型和剪枝后模型的结构差异\n",
    "    print(\"\\n比较原始模型和剪枝后的模型结构差异:\")\n",
    "    print(f\"原始模型的 fc1 输出特征数: {original_model.fc1.out_features}\")\n",
    "    print(f\"剪枝后模型的 fc1 输出特征数: {pruned_model.fc1.out_features}\")\n",
    "    print(f\"加载剪枝后模型的 fc1 输出特征数: {loaded_pruned_model.fc1.out_features}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
